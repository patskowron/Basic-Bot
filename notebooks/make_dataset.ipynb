{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse the Discord Training Data\n",
    "I got a historic record of K's Discord conversations. Currently, I do not have data on the conversations from other users which K's was replying too. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "#Load the Data\n",
    "with open('../data/Kenny-discord-convos.txt', encoding='utf-8') as f:\n",
    "    BasicBot_text = f.readlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am filtering any automatically generated join and leaving message. I am also replacing any website link with \"<HTTP>\" which I plan to use later to substitute in a relevent website. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parse the Data\n",
    "BasicBot_text = [ x for x in BasicBot_text if \"has joined the channel\" not in x] #Remove channel joins\n",
    "BasicBot_text = [ x for x in BasicBot_text if \"has left the channel\" not in x] #Remove channel exits\n",
    "BasicBot_text = [ re.sub(\"<https.*?>\", \"<HTTPS>\", x) for x in BasicBot_text] #Replace any http links with <HTTPS>\n",
    "#BasicBot_text = [re.sub(\"\\n$\", \"<|endoftext|>\", x) for x in BasicBot_text] #Signify the end of each message <|endoftext|>\n",
    "#BasicBot_text = [\"<|startoftext|>\" + x for x in BasicBot_text] #Signify the start of each message (<|startoftext|>)\n",
    "BasicBot_text_size=len(\"\".join(BasicBot_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download and Parse a Reddit Conversation Corpus\n",
    "Lets add a little variety to the Basic-Bot using some random comments from Reddit conversations. I've noticed that with Kenny's training data alone, the model either overfits or is a random combination of messages strung together. Lets some percentage of extra text from random reddit messages to make the output more interesting. If we want to give the bot different \"personalities\" we can also try restrict to specific subreddits.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset already exists at C:\\Users\\Patryk\\.convokit\\downloads\\reddit-corpus-small\n"
     ]
    }
   ],
   "source": [
    "from convokit import Corpus, download\n",
    "corpus = Corpus(filename=download(\"reddit-corpus-small\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "inject_prop=0.2 #proportion of reddit text to add compared to discord training data\n",
    "char_count=0\n",
    "Reddit_content=[]\n",
    "while char_count < inject_prop * BasicBot_text_size: \n",
    "  newtext=corpus.random_utterance().text +\"\\n\"\n",
    "  char_count = char_count + len(newtext)\n",
    "  Reddit_content.append(newtext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "Reddit_content=[x for x in Reddit_content if \"[deleted]\" not in x] #Remove comments flagged as deleted\n",
    "Reddit_content=[x for x in Reddit_content if \"[removed]\" not in x] #Remove comments flagged as removed\n",
    "Reddit_content=[re.sub(r'https?:\\/\\/\\S*', '<HTTPS>', x, flags=re.MULTILINE) for x in Reddit_content] #Replace urls with a generic code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine Training Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final training data consists of the Discord and random Reddit comments shuffled together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "255632"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "train_dat = BasicBot_text + Reddit_content\n",
    "random.shuffle(train_dat)\n",
    "train_dat = \"\".join(train_dat)\n",
    "len(train_dat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../data/Training_data_input.pickle\"\n",
    "with open(path, 'wb') as f:\n",
    "    pickle.dump(train_dat, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
